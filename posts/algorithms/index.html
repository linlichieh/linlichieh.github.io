<!DOCTYPE html>
<html lang="en-us">
<title>Algorithms | Software</title>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.104.2" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="/css/index.css">
<link rel="canonical" href="/posts/algorithms/">
<link rel="alternate" type="application/rss+xml" href="" title="Software">

<header>
  
    <a href="/" class="title">Software</a>
  
  
</header>

<article>
  <header>
    <h1>Algorithms</h1>
    <time datetime="2017-05-20T18:56:16&#43;08:00">May 20, 2017</time>
  </header>
  <h1 id="bubble-sort">Bubble sort</h1>
<h3 id="complexity">Complexity</h3>
<ul>
<li>Time O(n^2)
<ul>
<li>bubble sort works by repeatedly swapping adjacent elements if they are in the wrong order, and it continues this process until the array is sorted. The number of swaps required to sort the array is proportional to the square of the number of elements in the array</li>
</ul>
</li>
<li>Space O(1)</li>
</ul>
<h3 id="use-case">Use case</h3>
<p>pros</p>
<ul>
<li>Educational purposes: easy to implement</li>
<li>memory efficiency: elements are swapped in place without using additional temporary storage</li>
<li>stable sorting
<ul>
<li>For example, consider a list of records that represent people, where each record has a name and an age. If we sort the list based on age, it is important to maintain the relative order of people with the same age, so that the order of their names is preserved in the sorted list. A stable sorting algorithm, such as merge sort, would ensure that the relative order of equal elements is preserved, while an unstable sorting algorithm, such as quick sort, would not.</li>
</ul>
</li>
</ul>
<p>cons</p>
<ul>
<li>bad performance: it does not deal well with a list containing a huge number of items.
<ul>
<li>not recommended for use in practical applications due to its poor time complexity of O(n^2)</li>
</ul>
</li>
</ul>
<h3 id="how-it-works">How it works</h3>
<p>Compare two values and move the larger value to the right in each iteration.
This way, the greatest value will end up on the right side in each iteration and will be locked and excluded from subsequent iterations.</p>
<h3 id="steps">Steps</h3>
<p>input</p>
<pre><code>7 5 6 1 3
</code></pre>
<p>i=4, j=0, j&amp;j+1, (<code>5&lt;7</code>, swap)</p>
<pre><code>7 5 6 1 3
5 7 6 1 3
j j
</code></pre>
<p>i=4, j=1, j&amp;j+1,  (<code>7&gt;6</code>, swap)</p>
<pre><code>5 7 6 1 3
5 6 7 1 3
  j j
</code></pre>
<p>i=4, j=2, j&amp;j+1,  (<code>7&gt;1</code>, swap)</p>
<pre><code>5 6 7 1 3
5 6 1 7 3
    j j
</code></pre>
<p>i=4, j=3, j&amp;j+1,  (<code>7&gt;3</code>, swap)</p>
<pre><code>5 6 1 7 3
5 6 1 3 7
      j j
</code></pre>
<p>i=3, j=0, j&amp;j+1,  (<code>5&lt;6</code>, do nothing)</p>
<pre><code>5 6 1 3 7
j j
</code></pre>
<p>i=3, j=1, j&amp;j+1,  (<code>6&gt;1</code>, swap)</p>
<pre><code>5 6 1 3 7
5 1 6 3 7
  j j
</code></pre>
<p>i=3, j=2, j&amp;j+1,  (<code>6&gt;3</code>, swap)</p>
<pre><code>5 1 6 3 7
5 1 3 6 7
    j j
</code></pre>
<p>i=2, j=0, j&amp;j+1,  (<code>5&gt;1</code>, swap)</p>
<pre><code>5 1 3 6 7
1 5 3 6 7
j j
</code></pre>
<p>i=2, j=1, j&amp;j+1,  (<code>5&gt;3</code>, swap)</p>
<pre><code>1 5 3 6 7
1 3 5 6 7
  j j
</code></pre>
<p>i=1, j=0, j&amp;j+1,  (<code>1&lt;3</code>, do nothing)</p>
<pre><code>1 3 5 6 7
j j
</code></pre>
<p>end</p>
<h1 id="insertion-sort">Insertion sort</h1>
<h3 id="complexity-1">Complexity</h3>
<ul>
<li>Time O(n^2)
<ul>
<li>insertion sort works by iterating over the elements in the array, and for each element, it shifts all the elements that are greater than it to the right in order to make room for it in its final position. The number of shifts required to insert each element is proportional to the number of elements that have already been processed</li>
</ul>
</li>
<li>Space O(1)</li>
</ul>
<h3 id="use-case-1">Use case</h3>
<p>pros</p>
<ul>
<li>nearly-sorted data
<ul>
<li>If data is almost sorted, then it can be very fast approaching O(N) and faster than Merge sort</li>
<li>online sorting: For example, consider a system that generates log events in real-time and needs to sort the events based on their timestamp. An online sorting algorithm, such as insertion sort, would be a good choice in this scenario, since it can sort the events as they are generated, rather than waiting until all the events have been generated before sorting the entire array.</li>
</ul>
</li>
<li>memory efficiency: n-place, which means no auxiliary storage is necessary</li>
<li>stable sorting</li>
<li>Insertion sort algorithm is far better than selection sort algorithm
<ul>
<li>Selection sort algorithm can be used for small data sets, unfortunately Insertion sort algorithm best suitable for it.</li>
</ul>
</li>
</ul>
<p>cons</p>
<ul>
<li>bad performance: it does not deal well with a list containing a huge number of items. O(n^2)</li>
</ul>
<h3 id="how-it-works-1">How it works</h3>
<p>Set a starting point (i) that moves forward in each iteration after comparing all the numbers prior to it and move the smaller number to the left.</p>
<h3 id="steps-1">Steps</h3>
<p>input</p>
<pre><code>7 8 5 2 4 6 3
</code></pre>
<p>i=1, j=1, j&amp;j-1 (<code>7&lt;8</code>, do nothing)</p>
<pre><code>7 8 5 2 4 6 3
i
j j
</code></pre>
<blockquote>
<p>i is a starting point and j will be reduced by 1 in each i loop to compare 2 values</p>
</blockquote>
<p>i=2, j=2, j&amp;j-1  (<code>8&gt;5</code>, swap)</p>
<pre><code>7 8 5 2 4 6 3
7 5 8 2 4 6 3
  i
  j j
</code></pre>
<p>i=2, j=1, j&amp;j-1  (<code>7&gt;5</code>, swap)</p>
<pre><code>7 5 8 2 4 6 3
5 7 8 2 4 6 3
  i
j j
</code></pre>
<p>i=3, j=3, j&amp;j-1  (<code>8&gt;2</code>, swap)</p>
<pre><code>5 7 8 2 4 6 3
5 7 2 8 4 6 3
    i
    j j
</code></pre>
<p>i=3, j=2, j&amp;j-1  (<code>7&gt;2</code>, swap)</p>
<pre><code>5 7 2 8 4 6 3
5 2 7 8 4 6 3
    i
  j j
</code></pre>
<p>i=3, j=1, j&amp;j-1  (<code>5&gt;2</code>, swap)</p>
<pre><code>5 2 7 8 4 6 3
2 5 7 8 4 6 3
    i
j j
</code></pre>
<p>i=4, j=4, j&amp;j-1  (<code>8&gt;4</code>, swap)</p>
<pre><code>2 5 7 8 4 6 3
2 5 7 4 8 6 3
      i
      j j
</code></pre>
<p>i=4, j=3, j&amp;j-1  (<code>7&gt;4</code>, swap)</p>
<pre><code>2 5 7 4 8 6 3
2 5 4 7 8 6 3
      i
    j j
</code></pre>
<p>i=4, j=2, j&amp;j-1  (<code>5&gt;4</code>, swap)</p>
<pre><code>2 5 4 7 8 6 3
2 4 5 7 8 6 3
      i
  j j
</code></pre>
<p>i=4, j=1, j&amp;j-1  (<code>2&lt;4</code>, do nothing)</p>
<pre><code>2 4 5 7 8 6 3
      i
j j
</code></pre>
<p>i=5, j=5, j&amp;j-1  (<code>8&gt;6</code>, swap)</p>
<pre><code>2 4 5 7 8 6 3
2 4 5 7 6 8 3
        i
        j j
</code></pre>
<p>i=5, j=4, j&amp;j-1  (<code>7&gt;6</code>, swap)</p>
<pre><code>2 4 5 7 6 8 3
2 4 5 6 7 8 3
        i
      j j
</code></pre>
<p>i=5, j=3, j&amp;j-1  (<code>5&lt;6</code>, do nothing)</p>
<pre><code>2 4 5 6 7 8 3
        i
    j j
</code></pre>
<p>i=5, j=2, j&amp;j-1  (<code>4&lt;5</code>, do nothing)</p>
<pre><code>2 4 5 6 7 8 3
        i
  j j
</code></pre>
<p>i=5, j=1, j&amp;j-1  (<code>2&lt;4</code>, do nothing)</p>
<pre><code>2 4 5 6 7 8 3
        i
j j
</code></pre>
<p>i=6, j=6, j&amp;j-1  (<code>8&gt;3</code>, swap)</p>
<pre><code>2 4 5 6 7 8 3
2 4 5 6 7 3 8
          i
          j j
</code></pre>
<p>i=6, j=5, j&amp;j-1  (<code>7&gt;3</code>, swap)</p>
<pre><code>2 4 5 6 7 3 8
2 4 5 6 3 7 8
          i
        j j
</code></pre>
<p>i=6, j=4, j&amp;j-1  (<code>6&gt;3</code>, swap)</p>
<pre><code>2 4 5 6 3 7 8
2 4 5 3 6 7 8
          i
      j j
</code></pre>
<p>i=6, j=3, j&amp;j-1  (<code>5&gt;3</code>, swap)</p>
<pre><code>2 4 5 3 6 7 8
2 4 3 5 6 7 8
          i
    j j
</code></pre>
<p>i=6, j=2, j&amp;j-1  (<code>4&gt;3</code>, swap)</p>
<pre><code>2 4 3 5 6 7 8
2 3 4 5 6 7 8
          i
  j j
</code></pre>
<p>i=6, j=1, j&amp;j-1  (<code>2&gt;3</code>, do nothing)</p>
<pre><code>2 3 4 5 6 7 8
          i
j j
</code></pre>
<p>end</p>
<h1 id="selection-sort">Selection sort</h1>
<h3 id="complexity-2">Complexity</h3>
<ul>
<li>Time O(n^2)
<ul>
<li>selection sort works by repeatedly finding the minimum element in the unsorted portion of the array and swapping it with the first unsorted element. The number of swaps required to sort the array is proportional to the square of the number of elements in the array</li>
</ul>
</li>
<li>Space O(1)</li>
</ul>
<h3 id="use-cases">Use cases</h3>
<p>pros</p>
<ul>
<li>memory efficiency: in-place algorithm. It does not require a lot of space for sorting</li>
</ul>
<p>cons</p>
<ul>
<li>bad performance: It performs poorly when working on huge lists. O(n^2)</li>
<li>unstable algorithm</li>
</ul>
<h3 id="how-it-works-2">How it works</h3>
<p>Set a starting point (i) to find the minimum after it and swap it with the minimum.
Lock the minimum and move the start point forward in each iteration.</p>
<h3 id="steps-2">Steps</h3>
<p>input</p>
<pre><code>2 8 1 3 9
</code></pre>
<p>i=0, minIdx=0, j=0, j+1 (<code>2&lt;8</code>, do nothing)</p>
<pre><code>2 8 1 3 9
i j
m
</code></pre>
<p>i=0, minIdx=0, j=1, j+1 (<code>2&gt;1</code>, to be updated)</p>
<pre><code>2 8 1 3 9
i   j
m
</code></pre>
<p>i=0, minIdx=2, j=1, j+1 (update minIdx)</p>
<pre><code>2 8 1 3 9
i   j
    m
</code></pre>
<p>i=0, midIdx=2, j=2, j+1 (<code>1&lt;3</code>, do nothing)</p>
<pre><code>2 8 1 3 9
i   m j
</code></pre>
<p>i=0, midIdx=2, j=3, j+1 (<code>1&lt;9</code>, do nothing)</p>
<pre><code>2 8 1 3 9
i   m   j
</code></pre>
<p>i=0, midIdx=2, j=3, j+1 (end of iteration, swap min with i)</p>
<pre><code>1 8 2 3 9
m   i   j
</code></pre>
<p>i=1, midIdx=1, j=1, j+1 (<code>8&gt;2</code>, to be updated)</p>
<pre><code>1 8 2 3 9
  m j
  i
</code></pre>
<p>i=1, midIdx=2, j=1, j+1 (update minIdx)</p>
<pre><code>1 8 2 3 9
  i j
    m
</code></pre>
<p>i=1, midIdx=2, j=2, j+1 (<code>2&lt;3</code>, do nothing)</p>
<pre><code>1 8 2 3 9
  i m j
</code></pre>
<p>i=1, midIdx=2, j=3, j+1 (<code>2&lt;9</code>, do nothing)</p>
<pre><code>1 8 2 3 9
  i m   j
</code></pre>
<p>i=1, midIdx=2, j=3, j+1 (end of iteration, swap min with i)</p>
<pre><code>1 2 8 3 9
  m i   j
</code></pre>
<p>i=2, midIdx=2, j=2, j+1 (<code>8&gt;3</code>, to be updated)</p>
<pre><code>1 2 8 3 9
    i j
    m
</code></pre>
<p>i=2, midIdx=3, j=2, j+1 (update midIdx)</p>
<pre><code>1 2 8 3 9
    i j
      m
</code></pre>
<p>i=2, midIdx=3, j=3, j+1 (<code>3&lt;9</code>, do nothing)</p>
<pre><code>1 2 8 3 9
    i m j
</code></pre>
<p>i=2, midIdx=3, j=3, j+1 (end of iteration, swap min with i)</p>
<pre><code>1 2 3 8 9
    m i j
</code></pre>
<p>i=3, midIdx=3, j=3, j+1 (<code>8&lt;9</code>, do nothing)</p>
<pre><code>1 2 3 8 9
      i j
      m
</code></pre>
<p>i=3, midIdx=3, j=3, j+1 (end of iteration, swap min with i)</p>
<pre><code>1 2 3 8 9
      i j
      m
</code></pre>
<p>end</p>
<h1 id="merge-sort">Merge sort</h1>
<h3 id="complexity-3">Complexity</h3>
<ul>
<li>Time O(n log(n))
<ul>
<li>merge sort works by dividing the array into two halves, sorting each half separately, and then merging the two sorted halves back together. The merging process takes linear time proportional to the number of elements in the array, while the sorting process takes logarithmic time proportional to the number of elements in each half. The overall time complexity of merge sort is O(n log n) because the number of elements in each half is reduced by half with each iteration of the sorting process.</li>
</ul>
</li>
<li>Space O(n)
<ul>
<li>Merge sort has a space complexity of O(n) because it uses an auxiliary array of size n to store the sorted elements during the merging step.</li>
<li>merge sort requires an auxiliary array to store the result of each merging step. The size of this array is proportional to the number of elements in the array, which means that the space complexity of merge sort is O(n). Additionally, merge sort requires a small amount of additional memory to store the recursive call stack, but this memory usage is typically not significant compared to the size of the auxiliary array.</li>
</ul>
</li>
</ul>
<h3 id="use-cases-1">Use cases</h3>
<p>pros</p>
<ul>
<li>Good performance
<ul>
<li>It is efficient for both small and large data sets, with a time complexity of O(n log n) even though worst case occurs</li>
<li>widely used in practice due to its efficient time complexity and stability</li>
</ul>
</li>
<li>Stable sorting</li>
<li>Parallel processing
<ul>
<li>can be implemented in a parallel way, which can greatly speed up sorting large data set</li>
</ul>
</li>
<li>External sorting
<ul>
<li>Merge sort is often used for external sorting, where the elements to be sorted are too large to be stored in memory all at once. In this case, the elements are divided into smaller chunks that can be sorted and merged in a two-pass process, where the first pass sorts the chunks and the second pass merges the sorted chunks into the final sorted array.</li>
</ul>
</li>
</ul>
<p>cons</p>
<ul>
<li>use more memory: it requires more memory to store the sublists, which can be a problem with a very large list</li>
<li>slow for small arrays</li>
<li>the algorithm does the whole process even the array is already sorted</li>
<li>Marginally slower than quick sort in practise</li>
</ul>
<h3 id="how-it-works-3">How it works</h3>
<p>Divide the array into two equal-sized sub-arrays until each sub-array contains only one element. Then, merge each sub-array back into one, in order of value, until complete.</p>
<h3 id="steps-3">Steps</h3>
<pre><code>                [6 5 3 1 8 7 2 4]
                  /           \
             [6 5 3 1]     [8 7 2 4]
              /    \         /    \
          [6 5]  [3 1]    [8 7]   [2 4]
          / \     / \      / \     / \
        [6] [5] [3] [1]  [8] [7] [2] [4]
           \ /    \ /     \ /     \ /
          [5 6]  [1 3]    [7 8]  [2 4]
              \  /          \   /
           [1 3 5 6]      [2 4 7 8]
                  \          /
               [1 2 3 4 5 6 7 8 ]
</code></pre>
<h1 id="quick-sort">Quick sort</h1>
<h3 id="complexity-4">Complexity</h3>
<ul>
<li>Time
<ul>
<li>(avg) O(n log(n))
<ul>
<li>The pivot is then used as a pivot for recursive calls on the two partitioned subarrays. When the pivot is chosen optimally and the array is randomly shuffled, the size of each partition is roughly equal, which results in a good average time complexity of O(n log n)</li>
</ul>
</li>
<li>(worse) O(n^2)
<ul>
<li>if the pivot is always chosen poorly, the time complexity of quick sort can be O(n^2), which makes its time complexity heavily dependent on the choice of pivot.</li>
</ul>
</li>
</ul>
</li>
<li>Space O(log(n))
<ul>
<li>The space complexity of quick sort is O(log n) in the average case, and O(n) in the worst case, where n is the number of elements in the array. This is because quick sort uses a recursive approach, where each recursive call uses a small amount of memory proportional to the size of the call stack. In the average case, when the pivot is chosen optimally and the array is randomly shuffled, the size of each partition is roughly equal, which results in a good average space complexity of O(log n). However, in the worst case, if the pivot is always chosen poorly, the size of one partition can be much larger than the other, which results in a call stack with a height proportional to the number of elements in the array, resulting in a worst-case space complexity of O(n)</li>
</ul>
</li>
</ul>
<h3 id="use-case-2">Use case</h3>
<p>pros</p>
<ul>
<li>Good performance
<ul>
<li>Quick sort is highly efficient among all sorting algorithms</li>
<li>widely used in practice</li>
<li>fast in most cases</li>
</ul>
</li>
<li>memory usage: O(log(n))
<ul>
<li>It is an in-place algorithm since it just requires a modest auxiliary stack. O(log(n))</li>
<li>quicksort requires little space</li>
</ul>
</li>
<li>Parallel processing
<ul>
<li>during each call of QUICKSORT, the array is partitioned into two parts and each part is solved recursively. Sorting the smaller arrays represents two completely independent subproblems that can be solved in parallel. Therefore, one way to parallelize quicksort is to execute it initially on a single process; then, when the algorithm performs its recursive calls (lines 14 and 15), assign one of the subproblems to another process. Now each of these processes sorts its array by using quicksort and assigns one of its subproblems to other processes</li>
</ul>
</li>
</ul>
<p>cons</p>
<ul>
<li>unstable sorting
<ul>
<li>it swaps non-adjacent elements.</li>
</ul>
</li>
<li>worse case: O(N^2)
<ul>
<li>if you always choose the first or last array element as the pivot, then Quicksort takes N^2 time to sort an already-sorted array</li>
<li>slowest sort if things are already in reverse-order O(n^2)</li>
<li>On sorted and nearly-sorted arrays, its performance is slower than standard InsertionSort</li>
</ul>
</li>
</ul>
<h3 id="how-it-works-4">How it works</h3>
<p>Choose a pivot element, partition the data set into two sub-arrays recursively based on the pivot element,
and sort each sub-array by moving values less than the pivot to the left and values greater than the pivot to the right.
When all sub-arrays contain only one element, the entire array is considered sorted.</p>
<h3 id="steps-4">Steps</h3>
<pre><code>j
6 3 7 5 1 2 [4]    6 &gt; 4
i

  j
6 3 7 5 1 2 [4]     3 &lt; 4
i

  j
3 6 7 5 1 2 [4]     swap
i

  j
3 6 7 5 1 2 [4]     i+1
  i

    j
3 6 7 5 1 2 [4]     7 &gt; 4
  i

      j
3 6 7 5 1 2 [4]     5 &gt; 4
  i

        j
3 6 7 5 1 2 [4]     1 &lt; 4
  i

        j
3 1 7 5 6 2 [4]     swap
  i

        j
3 1 7 5 6 2 [4]     i+1
    i

          j
3 1 7 5 6 2 [4]
    i

          j
3 1 7 5 6 2 [4]     2 &lt; 4
    i

          j
3 1 2 5 6 7 [4]     swap
    i

          j
3 1 2 5 6 7 [4]     i+1
      i

             j
3 1 2 5 6 7 [4]     loop ends
      i
</code></pre>
<p>swap pivot and i</p>
<pre><code>3 1 2 [4] 6 7 5
</code></pre>
<p>Now, all elements that are less than the pivot are before it, and all elements that are greater than the pivot are after it.</p>
<p>Then divide the array into two and repeat the sorting process recursively for each part using the same steps</p>
<pre><code>[3 1 2]  4  [6 7 5]
</code></pre>
<p>and so on&hellip;</p>
<h1 id="heap-sort">Heap sort</h1>
<h3 id="complexity-5">Complexity</h3>
<ul>
<li>Time O(n log(n))
<ul>
<li>it takes O(log n) time to maintain the min-heap property by swapping elements down the tree as necessary</li>
</ul>
</li>
<li>Space O(1)
<ul>
<li>The algorithm swaps elements within the original array, so no extra memory is needed to store intermediate results</li>
</ul>
</li>
</ul>
<h3 id="use-cases-2">Use cases</h3>
<p>pros</p>
<ul>
<li>Efficiency &amp; Consistency
<ul>
<li>a better average-case time complexity of O(n log n)</li>
</ul>
</li>
<li>Memory usage is less: it doesn&rsquo;t require any extra space to sort an array O(1)</li>
</ul>
<p>cons</p>
<ul>
<li>unstable sort: not a stable sorting algorithm</li>
<li>Expensive constant factors
<ul>
<li>In the case of Heapsort vs. Quicksort, it turns out that there are ways (median of 5, for example) to make Quicksort&rsquo;s worst cases very rare indeed</li>
<li>Also, maintaining a heap is not free.</li>
<li>Given an array with a normal distribution, Quicksort and Heapsort will both run in O(n log(n)). But Quicksort will execute faster because its constant factors are smaller than the constant factors for Heapsort</li>
<li>partitioning is faster than maintaining the heap</li>
</ul>
</li>
<li>Huge datasets
<ul>
<li>If your dataset is really huge and doesn&rsquo;t fit into memory, then merge sort works like a charm. It&rsquo;s frequently used in clusters where dataset can span over hundreds of machines</li>
</ul>
</li>
</ul>
<h3 id="how-it-works-5">How it works</h3>
<ol>
<li>Build an max-heap from given unsorted array with the same idea of <code>heapify</code> of data structure <code>heap</code></li>
<li>In each iteration, swap the maximum with the last element, then run <code>heapify</code> to sort unsorted elements (except for the elements that are shifted to the right)</li>
</ol>
<h3 id="step-1---make-unsorted-array-a-max-heap">Step 1 - Make unsorted array a max-heap</h3>
<p>For an array with 9 elements, only the first 4 elements need to be heapified in order to get max-heap.
The reason why only the first 4 elements are needed is because the direction of <code>heapify</code> is downward, so the processes will take care of the whole heap</p>
<p>The example to explain above</p>
<pre><code>          -&gt; 8
           /   \
      -&gt; 27     14 &lt;-
        /  \   /  \
   -&gt; 18    9 36   55
     / \
   41   21

  [ 8, 27, 14, 18, 9, 36, 55, 41, 21 ]
    *   *   *   *
</code></pre>
<p>If there are 10 elements in the array:</p>
<pre><code>          -&gt; 8
           /   \
      -&gt; 27     14 &lt;-
        /  \   /  \
   -&gt; 18 -&gt; 9 36   55
     / \   /
   41  21 8

  [ 8, 27, 14, 18, 9, 36, 55, 41, 21, 8 ]
    *   *   *   *  *
</code></pre>
<p>Now, start to work, the input for the following demonstration</p>
<pre><code>   -&gt; 7
     / \
 -&gt; 1   2
   / \
  8   4

[7, 1, 2, 8, 4]
 *  *
</code></pre>
<blockquote>
<p>the first 2 elements will need to be heapified</p>
</blockquote>
<p>1st loop: swap value 8 with value 1 (heapify value 1)</p>
<pre><code>      7
     / \
 -&gt; 1   2
   / \
  8   4

      7
     / \
    8   2
   / \
  1   4
</code></pre>
<blockquote>
<p>the heapify is downward, so it stops here</p>
</blockquote>
<p>2nd loop: swap 7 with 8 (heapify value 7)</p>
<pre><code>   -&gt; 7
     / \
    8   2
   / \
  1   4

   -&gt; 8
     / \
    7   2
   / \
  1   4
</code></pre>
<p>max-heap is done</p>
<h3 id="step-2---sort-the-max-heap-by-shifting-maximum-to-the-right-in-each-iteration">Step 2 - Sort the max-heap by shifting maximum to the right in each iteration</h3>
<p>max-heap</p>
<pre><code>      8
     / \
    7   2
   / \
  1   4

  [8, 7, 2, 1, 4]
</code></pre>
<h5 id="1st-loop">1st loop</h5>
<p>shift index 0 with the last one (swap value 8 with value 4)</p>
<pre><code>      4
     / \
    7   2
   / \
  1   8

  [4, 7, 2, 1, 8]
</code></pre>
<p>do heapify for 0, 1, 2, 3 (without value 8), start with index 0</p>
<pre><code>   -&gt; 4
     / \
    7   2
   /
  1

  [4, 7, 2, 1, 8]
</code></pre>
<p>swap value 4 and value 7</p>
<pre><code>      7
     / \
 -&gt; 4   2
   /
  1

  [7, 4, 2, 1, 8]
</code></pre>
<p>do heapify, start from value 4</p>
<pre><code>      7
     / \
 -&gt; 4   2
   /
  1

  [7, 4, 2, 1, 8]
</code></pre>
<p>done (4 &gt; 1, do nothing)</p>
<h5 id="2nd-loop">2nd loop</h5>
<p>swap value 7 with value 1</p>
<pre><code>      1
     / \
    4   2
   /
  7

  [1, 4, 2, 7, 8]
</code></pre>
<p>do heapify for index 0, 1, 2 (without value 7), start from value 1</p>
<pre><code>   -&gt; 1
     / \
    4   2

  [1, 4, 2, 7, 8]
</code></pre>
<p>swap value 1 and value 4</p>
<pre><code>      4
     / \
 -&gt; 1   2

  [4, 1, 2, 7, 8]
</code></pre>
<h5 id="3rd-loop">3rd loop</h5>
<p>swap value 2 with value 4</p>
<pre><code>      2
     / \
    1   4

  [2, 1, 4, 7, 8]
</code></pre>
<p>do heapify for index 0, 1 (without value 4), start from value 2</p>
<pre><code>   -&gt; 2
     /
    1

  [2, 1, 4, 7, 8]
</code></pre>
<p>done (2 &gt; 1, do nothing)</p>
<h5 id="4th-loop">4th loop</h5>
<p>swap value 1 and value 2</p>
<pre><code>      1
     /
    2

  [1, 2, 4, 7, 8]
</code></pre>
<p>done (no need to do heapify)</p>
<h1 id="binary-search">binary search</h1>
<h3 id="remove-a-node-in-bst">Remove a node in BST</h3>
<p><code>insert</code> and <code>lookup</code> a node is quite easy, but removing a node is complex.</p>
<p>3 cases</p>
<ul>
<li>no child: just delete</li>
<li>1 child: replace the targetted node with its child node</li>
<li>2 children: there are 2 options to do that (so that the tree can continue to follow the rules of BST).
<ul>
<li>find the minimum value in right subtree, assign it to the node we want to delete (most common)</li>
<li>find the maximum value in left subtree, assign it to the node we want to delete</li>
</ul>
</li>
</ul>
<p>no child</p>
<pre><code>         100                                100
        /    \                             /
      75     125 &lt;- remove      =&gt;       75
</code></pre>
<p>1 child</p>
<pre><code>                100                         100
                /                           /
    remove -&gt; 75              =&gt;          65
             /                            /\
           65                           60  70
           /\
         60  70
</code></pre>
<p>1 child</p>
<pre><code>                3                         3
               / \                       / \
    remove -&gt; 1   4           =&gt;        2   4
               \
                2
</code></pre>
<p>1 child</p>
<pre><code>       3                             3
      / \                           / \
     1   5 &lt;- remove     =&gt;        1   4
        /
       4
</code></pre>
<p>2 children (leftmost in right subtree is 70)</p>
<pre><code>                    75                      75
                   /  \                    /  \
       remove -&gt; 65    85       =&gt;       70    85
                 /\                     /
               60  70                  60
</code></pre>
<p>2 children (leftmost in right subtree is 115)</p>
<pre><code>           100 &lt;- remove                     115
          /   \                             /  \
        75     125               =&gt;       75    125
              /   \                               \
            115    150                            150
</code></pre>
<p>2 children (leftmost in right subtree is 80)</p>
<pre><code>                     100                            100
                     /  \                          /   \
         remove -&gt; 75    125                     80     125
                 /    \          =&gt;            /    \
               65      85                    65      85
              /  \    /  \                  /  \       \
            60   70  80  95               60   70       95
</code></pre>
<p>2 children (leftmost in right subtree is 135)</p>
<pre><code>            100                             100
           /   \                           /   \
         75     125 &lt;- remove            75     135
               /   \             =&gt;            /   \
            115     150                      115   150
                   /   \                             \
                 135   175                           175
</code></pre>
<p>2 children (leftmost in right subtree is 34)</p>
<pre><code>            2                                   2
          /   \                               /   \
         0     33 &lt;- remove                 0      34
              /  \                                /  \
            25    40                            25    40
                 /  \           =&gt;                   /  \
               34    45                            36    45
                 \                                /  \
                  36                            35    39
                  /\
                35  39
</code></pre>
<h1 id="leetcode-23-merge-k-sorted-lists">Leetcode 23. Merge k Sorted Lists</h1>
<p>Candidate solutions</p>
<ul>
<li>Brute Force</li>
<li>Compare one by one</li>
<li>Priority Queue (min-heap)</li>
<li>Merge lists one by one</li>
<li>Merge with Divide And Conquer</li>
</ul>
<p>Symbol explanation</p>
<ul>
<li><code>n</code> = average size of each list</li>
<li><code>k</code> =  the number of linked lists</li>
<li><code>N</code> = <code>n</code> * <code>k</code> (the total number of nodes)</li>
</ul>
<h3 id="solution---brute-force">Solution - Brute Force</h3>
<ul>
<li>Algorithm
<ul>
<li>combining all the linked lists into a single list and then sorting the merged list</li>
</ul>
</li>
<li>Time
<ul>
<li><code>O(NlogN)</code> = <code>O(nk log nk)</code>
<ul>
<li>traversing all the linked lists and collect the values of the nodes into an array <code>O(N)</code></li>
<li>Sorting <code>O(N log N)</code></li>
<li>total: <code>O(N)</code> + <code>O(N log N)</code> = <code>O(N log N)</code></li>
</ul>
</li>
</ul>
</li>
<li>Space
<ul>
<li><code>O(N)</code>
<ul>
<li>create a new linked list to store the merged and sorted list <code>O(N)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="solution---compare-one-by-one">Solution - Compare one by one</h3>
<ul>
<li>Algorithm
<ul>
<li>Iterate <code>k</code> sorted lists, then compare every head of each list and get the node with the smallest value and put it into the final linked list</li>
</ul>
</li>
<li>Time
<ul>
<li><code>O(kN)</code> = <code>O(nk^2)</code>
<ul>
<li>Explanation 1
<ul>
<li>Almost every node in final linked costs <code>O(k)</code> (<code>k-1</code> times comparison)</li>
<li>There are <code>N</code> nodes in the final linked list.</li>
<li>Therefore, it costs <code>O(kN)</code> in total</li>
</ul>
</li>
<li>Explanation 2
<ul>
<li>The worst case occurs when all the input lists have a length of <code>n</code>, and we need to iterate over all <code>k</code> lists in each iteration to find the minimum element.</li>
<li>In each iteration, we need to compare the first element of each list to find the minimum element, which takes <code>O(k)</code> time. Since we need to perform this operation <code>nk</code> times to construct the merged list, the total time complexity is <code>O(nk^2)</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Space
<ul>
<li><code>O(1)</code>
<ul>
<li>We only need a constant amount of extra space to store the merged list. We do not need to create any new data structures, as we can modify the pointers of the existing nodes to create the merged list.</li>
</ul>
</li>
<li><code>O(N)</code>
<ul>
<li>Creating a new linked list costs <code>O(N)</code> space.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="solution---priority-queue-min-heap">Solution - Priority Queue (min-heap)</h3>
<ul>
<li>Algorithm
<ul>
<li>Add the first <code>k</code> nodes from each lists</li>
<li>Iterate k sorted lists
<ul>
<li>Extract the minimum node from the queue (the node with the smallest value)</li>
<li>Append the extracted node to the merged list</li>
<li>If the extracted node has a next node in its original list, add the next node to the queue</li>
<li>Until the heap is empty</li>
</ul>
</li>
</ul>
</li>
<li>Time
<ul>
<li><code>O(N log k)</code> = <code>O(nk log k)</code>
<ul>
<li>add the first node of each list to the heap in <code>O(k)</code> time</li>
<li>then do iteration
<ul>
<li>extract the minimum node and add the next node from the same list to the heap in <code>O(logk)</code> time</li>
<li>repeat above for each of the <code>N</code> nodes</li>
</ul>
</li>
<li>total: <code>O(k)</code> + <code>N</code> * <code>O(logk)</code> = <code>O(N log k)</code></li>
<li>Note: we only have <code>k</code> elements in the heap at any given time, because in each iteration we only add 1 element into heap</li>
</ul>
</li>
</ul>
</li>
<li>Space
<ul>
<li><code>O(k)</code>
<ul>
<li>We only need a priority queue to store the <code>k</code> nodes, one from each of the <code>k</code> lists, at any given time.</li>
</ul>
</li>
<li><code>O(N)</code>
<ul>
<li>Creating a new linked list costs <code>O(N)</code> space.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="solution---merge-lists-one-by-one">Solution - Merge lists one by one</h3>
<ul>
<li>Algorithm
<ul>
<li>merging two lists at a time until we merge all <code>k</code> lists into one sorted list</li>
</ul>
</li>
<li>Time
<ul>
<li><code>O(kN)</code> = <code>O(nk^2)</code>
<ul>
<li>All lists have the same size <code>n</code>. At the first step of the algorithm we merge 2 lists with <code>O(n)</code> and get a list with size <code>2n</code>.</li>
<li>At the second step we merge a <code>2n</code> list with a <code>n</code> list, in the worse case we have to visit <code>2n</code> nodes and get a list with size <code>3n</code>.</li>
<li>And so on. At the end we have <code>n + 2n + ... + kn = n(1+2 + .. + k) = n * ((k + 1)*k) / 2</code> = <code>n * (k^2 + k)/2</code> = <code>nk^2/2</code> + <code>nk/2</code> = <code>nk^2</code>
<ul>
<li>e.g. 1+2+3+4+5 = ((1+5) * 5) / 2</li>
</ul>
</li>
<li>So in average we have here <code>O(nk^2)</code>.</li>
</ul>
</li>
</ul>
</li>
<li>Space
<ul>
<li><code>O(1)</code>
<ul>
<li>modify the pointers of the existing nodes to create the merged list, without creating any new data structures.</li>
</ul>
</li>
<li><code>O(N)</code>
<ul>
<li>Creating a new linked list costs <code>O(N)</code> space.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="solution---merge-with-divide-and-conquer">Solution - Merge with Divide And Conquer</h3>
<ul>
<li>Algorithm
<ul>
<li>recursively merging the linked lists in a divide-and-conquer manner, until we merge all <code>k</code> lists into one sorted list</li>
<li>The idea behind the solution is to use a divide-and-conquer approach to merge the <code>k</code> sorted lists.  We first divide the <code>k</code> lists into two halves, recursively sort each half, and then merge the two halves.  This process is repeated until we have a single merged list.</li>
</ul>
</li>
<li>Time
<ul>
<li><code>O(N log k)</code> = <code>O(nk log k)</code>
<ul>
<li>We divide the <code>k</code> lists into half at each level. Then we will have <code>O(logk)</code> levels in total.</li>
<li>Each level takes <code>O(nk)</code> time in total</li>
<li>Since we have <code>O(logk)</code> levels and each level take <code>O(nk)</code> time, the total time complexity is <code>O(nk logk)</code>.</li>
</ul>
</li>
</ul>
</li>
<li>Space
<ul>
<li>O(1)
<ul>
<li>We can merge two sorted linked lists in <code>O(1)</code> space.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The demonstration of time complexity</p>
<pre><code>assuming, k = 8, n = 4

            n n n n n n n n n
             /             \
        n n n n           n n n n     =&gt; each pair take O(8n), each level takes O(8n) = O(kn)
         /   \             /   \
      n n     n n       n n     n n   =&gt; each pair take O(4n), each level takes O(8n) = O(kn)
      / \     / \       / \     / \
     n   n   n   n     n   n   n   n  =&gt; each pair takes O(2n), each level takes O(8n) = O(kn)
</code></pre>
<blockquote>
<p>The total time complexity is <code>O(kn) + O(kn) + O(kn)</code> = <code>O(kn) * the number of levels</code> = <code>O(kn) * O(log k)</code> = <code>O(nk log k)</code></p>
</blockquote>
<blockquote>
<ul>
<li>chatGPT</li>
<li><a href="https://leetcode.com/problems/merge-k-sorted-lists/editorial/">https://leetcode.com/problems/merge-k-sorted-lists/editorial/</a></li>
</ul>
</blockquote>
<h1 id="traversal">Traversal</h1>
<h3 id="bfs-breadth-first-search">BFS (Breadth First Search)</h3>
<p>tree</p>
<pre><code>            9
          /   \
        4      20
       / \    /  \
      1   6  15   170
</code></pre>
<p>traversal</p>
<pre><code>9 4 20 1 6 15 170
</code></pre>
<blockquote>
<p>time complexity is O(n)</p>
</blockquote>
<p>pros</p>
<ul>
<li>If Solution exists BFS will definitely find it</li>
<li>easy to find the shortest path between two nodes because BFS visits nodes level-by-level, and it will find the destination node as soon as it is encountered</li>
<li>Never get trapped in unwanted nodes without the solution</li>
<li>useful for finding the maximum or minimum value in a BST</li>
</ul>
<p>cons</p>
<ul>
<li>BFS uses more memory than DFS because it needs to keep track of all nodes at the same level before moving on to deeper levels (e.g. queue)</li>
<li>Slower for deep trees (a higher time complexity than DFS because it visits all nodes at the same level before moving on to the next level)</li>
</ul>
<p>use cases</p>
<ul>
<li>find BT nods</li>
<li>search engine use BFS to build index</li>
<li>GPS navigation</li>
<li>Network problems: BFS can be used to find the shortest path in a network. For example, it can be used to find the shortest path between two nodes in a computer network, or to find the shortest path between two nodes in a social network.</li>
<li>Map problems: BFS can be used to find the shortest path in a map.</li>
<li>recommendation engine
<ul>
<li>on Amazon: what types of items are related or the closet relation to the last book I bought</li>
<li>on FB: what types of friend requests I should be recommended</li>
</ul>
</li>
</ul>
<h3 id="dfs-depth-first-search">DFS (Depth First Search)</h3>
<p>tree</p>
<pre><code>            9
          /   \
        4      20
       / \    /  \
      1   6  15   170
</code></pre>
<p>traversal</p>
<pre><code>9 4 1 6 20 15 170
</code></pre>
<blockquote>
<p>The time complexity is O(n) regardless of the traversal method used</p>
</blockquote>
<p>pros</p>
<ul>
<li>Memory efficiency: DFS uses a stack to store nodes to be processed, which only requires a limited amount of memory compared to BFS, which uses a queue and requires more memory.</li>
<li>Faster for deep trees because it can quickly reach the bottom-most level of a tree.</li>
<li>Faster in finding a solution: DFS can backtrack and quickly eliminate branches that do not contain a solution especially if the solution is located deep in the graph. For example, consider a problem where you need to find a specific node in a large graph. With DFS, you can start from a node and follow its edges to explore deeper into the graph. If you find the solution, you can stop the search and return the result.</li>
</ul>
<p>cons</p>
<ul>
<li>Can get stuck in an infinite loop: If there are cycles in the graph, DFS may get stuck in an infinite loop, whereas BFS is able to avoid this issue.</li>
<li>Cannot guarantee to find a solution. For example, if we search for Starbucks from my location, it will definitely find one soon. but if we search for a museum, then it will take more time and might not guarantee to find one.</li>
<li>Cannot find the minimal solution if two solutions are available</li>
<li>Slower for shallow trees</li>
</ul>
<p>use cases</p>
<ul>
<li>If we perform DFS on unweighted graph, then it will create minimum spanning tree for all pair shortest path tree</li>
<li>Using DFS we can find path between two given vertices u and v.</li>
<li>topological sorting: Topological sorting is a linear ordering of the vertices in a directed acyclic graph (DAG) (e.g. A-&gt;B-&gt;C-&gt;D)
<ul>
<li>scheduling problems: can be used to schedule jobs from given dependencies among jobs</li>
</ul>
</li>
<li>cycle detection in graphs</li>
<li>solving puzzles with only one solution, such as a maze or a sudoku puzzle.</li>
<li>analysing networks e.g. testing if a graph is bipartite (a graph where vertices can be partitioned into two disjoint sets, with no edges connecting vertices in the same set. It&rsquo;s used in matching, scheduling, and network flow problems and can be easily tested for by coloring vertices with two colors and checking if no two adjacent vertices have the same color.)</li>
<li>on linkedin: If I have a connection someone, I can use DFS for what degree of connection with that person.</li>
</ul>
<h3 id="inorder-vs-preorder-vs-postorder">inorder vs preorder vs postorder</h3>
<p>tree</p>
<pre><code>            9
          /   \
        4      20
       / \    /  \
      1   6  15   170
</code></pre>
<p>inorder</p>
<pre><code>1 4 6 9 15 20 170
</code></pre>
<p>preorder</p>
<pre><code>9 4 1 6 20 15 170
</code></pre>
<p>postorder</p>
<pre><code>1 6 4 15 170 20 9
</code></pre>
<p>use cases:</p>
<ul>
<li>If you know you need to explore the roots before inspecting any leaves, you pick pre-order because you will encounter all the roots before all of the leaves.</li>
<li>If you know you need to explore all the leaves before any nodes, you select post-order because you don&rsquo;t waste any time inspecting roots in search for leaves.</li>
<li>If you know that the tree has an inherent sequence in the nodes, and you want to flatten the tree back into its original sequence, than an in-order traversal should be used.</li>
</ul>
<h3 id="why-does-bfs-not-have-inorder-preorder-and-postorder-implementations-while-dfs-does">Why does BFS not have inorder, preorder, and postorder implementations, while DFS does?</h3>
<ul>
<li>BFS visits nodes in a level-by-level manner, which makes it difficult to visit the nodes in a specific order.</li>
<li>DFS has inorder, preorder, and postorder implementations because it can be used to traverse the tree and visit the nodes in a specific order by visiting the children of a node before visiting the node itself.</li>
</ul>
<h1 id="whats-the-pros-and-cons-of-recursive-solution">What&rsquo;s the pros and cons of recursive solution?</h1>
<p>pros</p>
<ul>
<li>reduce time complexity (if use recursion with memorisation)</li>
<li>cleaner code</li>
<li>Recursion is better at tree traversal
<ul>
<li>One of the more efficient ways to traverse these trees when looking for a specific leaf (or node) is by recursively following a single branch until the end of that branch until you find the value you are looking for.</li>
</ul>
</li>
</ul>
<p>cons</p>
<ul>
<li>use more memory (might cause stack overflow)
<ul>
<li>Because the function has to add to the stack with each recursive call and keep the values there until the call is finished</li>
</ul>
</li>
<li>can be slow (generally, compare to iteration)
<ul>
<li>it requires the allocation of a new stack frame</li>
<li>fibonacci question is a good example. Iteration is much faster than recursion, because iteration saves the value of each calculation for further use.</li>
</ul>
</li>
<li>more difficult to understand and debug</li>
</ul>
<h1 id="other-algorithms">Other algorithms</h1>
<ul>
<li>Tree sort
<ul>
<li>Steps
<ul>
<li>Create a binary search tree from an array</li>
<li>Do an in-order traversal</li>
</ul>
</li>
</ul>
</li>
<li>Radix sort
<ul>
<li>Time O(nk)</li>
<li>Space O(n+k)</li>
<li>used to sort integrers or strings</li>
</ul>
</li>
<li>Counting sort
<ul>
<li>Time O(n+k)</li>
<li>Space O(k)</li>
</ul>
</li>
</ul>
<h3 id="shortest-path-of-a-weighted-graph">shortest path (of a weighted graph)</h3>
<ul>
<li>Bellman-Ford
<ul>
<li>support negative weight</li>
</ul>
</li>
<li>Dijkstra</li>
</ul>
<h1 id="dynamic-programming">Dynamic Programming</h1>
<ul>
<li>= Divide &amp; Conquer + Memoisation</li>
</ul>
<h1 id="functional-programming">Functional Programming</h1>
<h1 id="interview">Interview</h1>
<h3 id="top-6-coding-interview-concepts">Top 6 Coding Interview Concepts</h3>
<ul>
<li>Heaps
<ul>
<li>min-heaps</li>
<li>max-heaps</li>
<li>K closest points to origin</li>
<li>network delay time</li>
<li>min cost to connect all points</li>
</ul>
</li>
<li>sliding window
<ul>
<li>Best time to buy/sell a stock</li>
</ul>
</li>
<li>Binary search
<ul>
<li>Guess number higher or lower</li>
<li>search a 2-D matrix</li>
<li>binary search</li>
</ul>
</li>
<li>DFS &amp; BFS
<ul>
<li>usually O(V+E), time &amp; space complexity</li>
<li>number of islands</li>
</ul>
</li>
<li>Recursion
<ul>
<li>includes Trees, Graphs, Backtracking, DP</li>
<li>N-queens</li>
</ul>
</li>
<li>Hash maps
<ul>
<li>Two sum</li>
</ul>
</li>
</ul>
<h3 id="the-10-most-important-concepts-for-coding-interviews">The 10 Most Important Concepts For Coding Interviews</h3>
<ul>
<li>logarithum</li>
<li>DFS/BFS</li>
<li>binary search</li>
<li>sliding window</li>
<li>recursion</li>
<li>inverting a binary tree and reverting a linked list</li>
<li>suffix trees</li>
<li>heaps</li>
<li>dynamic programming</li>
<li>sorting algorithum</li>
</ul>
<h4 id="ref">ref</h4>
<ul>
<li><a href="https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity">Big O Notation</a></li>
<li><a href="https://goo.gl/mKYH19">初學者學演算法｜從時間複雜度認識常見演算法（一）</a></li>
<li><a href="https://www.quora.com/What-are-the-advantages-of-using-BFS-over-DFS-or-using-DFS-over-BFS-What-are-the-applications-and-downsides-of-each">bfs vs dfs</a></li>
<li><a href="https://www.tutorialspoint.com/applications-of-dfs-and-bfs-in-data-structures">use cases of BFS/DFS</a></li>
<li><a href="https://medium.com/@williambdale/recursion-the-pros-and-cons-76d32d75973a">recursive</a></li>
<li><a href="https://stackoverflow.com/questions/9456937/when-to-use-preorder-postorder-and-inorder-binary-search-tree-traversal-strate">preorder vs inorder vs postorder</a></li>
<li>chatGPT :D</li>
</ul>

</article>



</html>
